{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    MambaConfig, MambaForCausalLM, AutoTokenizer\n",
    ")\n",
    "from jinja2 import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "# model_path = './train_exp_1b/complete' # '/kaggle/input/prompt_reversal_hf/transformers/1b/1' # './train_exp_1b/complete'\n",
    "# model_path =\n",
    "# model_path = './train_exp_3b'\n",
    "model_path = './train_exp_4/complete'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = MambaForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    device_map=device, \n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "start_sub = '<|PROMPT|>'\n",
    "end_sub = '<|END_PROMPT|>'\n",
    "\n",
    "prompt = \"\"\"<|ORIGINAL_TEXT|>{{ original_text }}<|END_ORIGINAL_TEXT|>\n",
    "<|GENERATED_TEXT|>{{ rewritten_text }}<|END_GENERATED_TEXT|>\n",
    "<|PROMPT|>\"\"\"\n",
    "jinja_env = Environment()\n",
    "prompt_template = jinja_env.from_string(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"data/kaggle_3p_data/data/juanmerinobermejo/rewritten_texts_csv.csv\", encoding_errors='ignore', on_bad_lines='skip')\n",
    "\n",
    "test_df.insert(0, 'id', range(0, len(test_df)))\n",
    "test_df.rename(columns={'prompt': 'gt_rewrite_prompt'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[:5]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# loop through everything\n",
    "with tqdm(total=test_df.shape[0]) as pbar:\n",
    "    for idx, row in test_df.iterrows():\n",
    "        # generate input prompt\n",
    "        \n",
    "        prompt = prompt_template.render(\n",
    "            original_text=row['original_text'],\n",
    "            rewritten_text=row['rewritten_text']\n",
    "        )\n",
    "        \n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "        gen = model.generate(\n",
    "            input_ids,\n",
    "            # max_length=2048,\n",
    "            max_new_tokens=200    \n",
    "        )\n",
    "        out = tokenizer.batch_decode(gen)[0]\n",
    "        start_idx = out.find(start_sub)\n",
    "        end_idx = out.find(end_sub)\n",
    "        res = out[start_idx + len(start_sub): end_idx]\n",
    "        print(row['gt_rewrite_prompt'])\n",
    "        print(res)\n",
    "        test_df.loc[idx, 'rewrite_prompt'] = res # 'Improve this text'\n",
    "        \n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to score\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "st_model = SentenceTransformer('sentence-transformers/sentence-t5-base')\n",
    "\n",
    "def get_sharpened_cosine_similarity(text1, text2):\n",
    "    embeddings1 = st_model.encode(text1)\n",
    "    embeddings2 = st_model.encode(text2)\n",
    "    cosine_score = util.cos_sim(embeddings1, embeddings2)\n",
    "    # print(cosine_score) \n",
    "    return (cosine_score[0] ** 3).numpy()[0]\n",
    "\n",
    "def calc_prompt_similarity(row):\n",
    "    return get_sharpened_cosine_similarity(row['gt_rewrite_prompt'], row['rewrite_prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc and show score\n",
    "\n",
    "test_df['score'] = test_df.apply(lambda row: calc_prompt_similarity(row), axis=1)\n",
    "\n",
    "test_df['score'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df.iloc[0]['gt_rewrite_prompt'])\n",
    "print(test_df.iloc[0]['rewrite_prompt'])\n",
    "print(test_df.iloc[0]['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Model: `./train_exp_4/complete`\n",
    "\n",
    "LB Score: 0.72472763\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "\n",
    "./train_exp_4/complete\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_promt_reversal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
