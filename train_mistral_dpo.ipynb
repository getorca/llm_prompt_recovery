{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from peft import PeftModel, LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from datasets import load_dataset, Dataset\n",
    "import polars as pl\n",
    "from unsloth import FastLanguageModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "adapter_path = '/home/lawrence/Projects/my_models/mistral_pr_lora_over65'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems to be an issue with loading multiple adpaters with unsloth... and loading the unsloth adapater with regular model\n",
    "# 2 - might want to train \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "# Load the adapter.\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    adapter_path,\n",
    "    is_trainable=True,\n",
    "    adapter_name=\"train\",\n",
    ")\n",
    "# Load the adapter a second time, with a different name, which will be our reference model.\n",
    "model.load_adapter(adapter_path, adapter_name=\"reference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"mistral\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_prefix = \"Original Text:\"\n",
    "rewrite_prefix = \"Rewritten Text:\"\n",
    "response_start = \"The prompt was:\"\n",
    "sys_prompt = \"\"\"You are an expert in \"Reverse Prompt Engineering\". You are able to reverse-engineer prompts used to rewrite text. \n",
    "\n",
    "I will be providing you with an \"original text\" and \"rewritten text\". Please try to be as specific as possible and come up with a prompt that is based on the tone, style, and any other properties you consider relevant.\"\"\"\n",
    "\n",
    "def format_prompts(x):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"{sys_prompt}\\n{orig_prefix} {x['original_text']}\\n{rewrite_prefix} {x['rewritten_text']}\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"{response_start} \"}\n",
    "    ]\n",
    "    output = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = False).rstrip('</s>')\n",
    "    return {\"prompt\": output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pl.read_csv('./data/predictions/combined-filtered_*.csv', ignore_errors=True)\n",
    "df = pl.read_csv('./data/exp_test/multi_n_with_lora/multi_n_selected.csv', ignore_errors=True)\n",
    "dataset = Dataset.from_list(df.to_dicts())\n",
    "dataset = dataset.map(format_prompts)\n",
    "dataset.remove_columns_(['', 'original_text', 'rewritten_text', 'gt_rewrite_prompt', 'old_rewrite_prompt','score','prompt_select','input','rewrite_prompts','rewrite_prompt_1','rewrite_prompt_2','score_1','score_2','prompt_select_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs dpo code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained('/home/lawrence/Projects/my_models/mistral_pr_lora_over65v2b')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
