{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67121,"databundleVersionId":7806901,"sourceType":"competition"}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/transformers -U\n!pip install accelerate\n!pip install -i https://pypi.org/simple/ bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-03-06T22:36:34.038945Z","iopub.execute_input":"2024-03-06T22:36:34.039323Z","iopub.status.idle":"2024-03-06T22:37:47.381149Z","shell.execute_reply.started":"2024-03-06T22:36:34.039294Z","shell.execute_reply":"2024-03-06T22:37:47.379932Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/huggingface/transformers\n  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-ifnl5_iz\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-ifnl5_iz\n  Resolved https://github.com/huggingface/transformers to commit 965cf677695dd363285831afca8cf479cf0c600c\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.39.0.dev0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (2024.2.2)\nBuilding wheels for collected packages: transformers\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for transformers: filename=transformers-4.39.0.dev0-py3-none-any.whl size=8662804 sha256=22971fcc58f14b00f8347774542f673e88a62037875f1a531c3eae026eff8a14\n  Stored in directory: /tmp/pip-ephem-wheel-cache-j1yn5bbs/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\nSuccessfully built transformers\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.38.1\n    Uninstalling transformers-4.38.1:\n      Successfully uninstalled transformers-4.38.1\nSuccessfully installed transformers-4.39.0.dev0\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.27.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nLooking in indexes: https://pypi.org/simple/\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.42.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.11.4)\nRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from scipy->bitsandbytes) (1.26.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch \n\n\naccess_token_read = UserSecretsClient().get_secret(\"HUGGINGFACE_ACCESS_TOKEN\")\nlogin(token = access_token_read)\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-03-06T22:38:24.190686Z","iopub.execute_input":"2024-03-06T22:38:24.191046Z","iopub.status.idle":"2024-03-06T22:38:30.136608Z","shell.execute_reply.started":"2024-03-06T22:38:24.191014Z","shell.execute_reply":"2024-03-06T22:38:30.135494Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"quantization_config = BitsAndBytesConfig(load_in_4bit=True)\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-7b\", \n    low_cpu_mem_usage=True, \n    quantization_config=quantization_config, \n    device_map=\"auto\"\n)#.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T22:38:43.193376Z","iopub.execute_input":"2024-03-06T22:38:43.194001Z","iopub.status.idle":"2024-03-06T22:41:16.464714Z","shell.execute_reply.started":"2024-03-06T22:38:43.193967Z","shell.execute_reply":"2024-03-06T22:41:16.463692Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a336d4a8273040658851d9aa07e99304"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bac33223a60845d39a463f154f38608e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69219aa27261491abf34e8c3c09bd786"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b387f74236aa40e2a34afb24184fdc2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b70cfa0656b34f05ad959fbad3838393"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"697a241497634132acc1d93b3a00fb9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0c3d5c3375a4a0ba03225aa702c959d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0616479c1c9c42fb8e879c5ea33cafe7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"655494ec3c4744e48e9872fb4e7dc9e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dbc607eb1e44f479c212fe5493180cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"362cdae16aa649baa2c405790c08f8d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f372c37b2e44bffad881de91a1b4a97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35f950d988a1410e97b7ae861be5b4d9"}},"metadata":{}}]},{"cell_type":"code","source":"test_set_df = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the model\ninput_text = \"What is the best thing about Kaggle?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submission\nsubmission.to_csv(\"submission.csv\", index=False, header=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# single shot\n\nchat = [\n    { \"role\": \"user\", \"content\": \"\"\"You are a helpful assistant. It is your job to think of a prompt that could have been used to write the output from a user.\\n\n    For example:\n    original_text: I love to play basketball\n    rewritten_text: My favorite sport is basketball!\n    rewrite_prompt: Improve the text\n    \n    \n    original_text: The competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text.  Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. Expect roughly 2,000 original texts in the test set.\n    rewritten_text: Here is your shanty: (Verse 1) The text is rewritten, the LLM has spun, With prompts so clever, they've been outrun. The goal is to find, the prompt so bright, To crack the code, and shine the light. (Chorus) Oh, this is a code competition, my dear, With text and prompts, we'll compete. Two thousand texts, a challenge grand, To guess the prompts, hand over hand.(Verse 2) The original text, a treasure lost, The rewrite prompt, a secret to be\n    rewrite_prompt:\n    \"\"\"\n},\n]\nprompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n\nprompt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"\"\"You are a helpful assistant. It is your job to think of a prompt that could have been used to create the `rewritten_text` from `original_text` and figure out the `rewrite_prompt`.\n\n    For example:\n    original_text: I love to play basketball\n    rewritten_text: My favorite sport is basketball!\n    rewrite_prompt: Improve the text\n    \n    Now it's your turn, following the example above, complete the example below and `rewrite_prompt:` with the text you best think fits. Remember marks are taken off for getting it wrong. Do not add any extra text.\n    original_text: The competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text.  Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. Expect roughly 2,000 original texts in the test set.\n    rewritten_text: Here is your shanty: (Verse 1) The text is rewritten, the LLM has spun, With prompts so clever, they've been outrun. The goal is to find, the prompt so bright, To crack the code, and shine the light. (Chorus) Oh, this is a code competition, my dear, With text and prompts, we'll compete. Two thousand texts, a challenge grand, To guess the prompts, hand over hand.(Verse 2) The original text, a treasure lost, The rewrite prompt, a secret to be\n    rewrite_prompt:\n    \"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids, max_length=500)\nprint(tokenizer.decode(outputs[0]))","metadata":{"execution":{"iopub.status.busy":"2024-03-06T22:44:25.340175Z","iopub.execute_input":"2024-03-06T22:44:25.341444Z","iopub.status.idle":"2024-03-06T22:44:36.946568Z","shell.execute_reply.started":"2024-03-06T22:44:25.341411Z","shell.execute_reply":"2024-03-06T22:44:36.945517Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<bos>You are a helpful assistant. It is your job to think of a prompt that could have been used to create the `rewritten_text` from `original_text` and figure out the `rewrite_prompt`.\n\n    For example:\n    original_text: I love to play basketball\n    rewritten_text: My favorite sport is basketball!\n    rewrite_prompt: Improve the text\n    \n    Now it's your turn, following the example above, complete the example below and `rewrite_prompt:` with the text you best think fits. Remember marks are taken off for getting it wrong. Do not add any extra text.\n    original_text: The competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text.  Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. Expect roughly 2,000 original texts in the test set.\n    rewritten_text: Here is your shanty: (Verse 1) The text is rewritten, the LLM has spun, With prompts so clever, they've been outrun. The goal is to find, the prompt so bright, To crack the code, and shine the light. (Chorus) Oh, this is a code competition, my dear, With text and prompts, we'll compete. Two thousand texts, a challenge grand, To guess the prompts, hand over hand.(Verse 2) The original text, a treasure lost, The rewrite prompt, a secret to be\n    rewrite_prompt:\n     glancing_prompt: Guess the prompt\n    ---->\n    ~\"The original text, a treasure lost,~\"\n    \n    ~\"The rewrite prompt, a secret to be~\",\n    \n    ~\"The goal is to find, the prompt so bright daz~d,~\"\n    \n    ~\"To crack the code, and shine the light.\"*~d daz,~\"\n    \n    ~\"Oh, this is a code competition, my dear,~\"\n    \n    ~\"With text and prompts, we'll compete daz.\"*~d daz daz.\"*~d.\"*~d.\"*~d.\"*~d.\"*~d.\"*~d.\"*~d.\"*~d.\"*~d.\"*~\n","output_type":"stream"}]}]}