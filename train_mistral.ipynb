{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from peft import PeftModel, LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from datasets import load_dataset, Dataset\n",
    "import polars as pl\n",
    "from unsloth import FastLanguageModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.4\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.688 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"mistral\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_prefix = \"Original Text:\"\n",
    "rewrite_prefix = \"Rewritten Text:\"\n",
    "response_start = \"The prompt was:\"\n",
    "sys_prompt = \"\"\"You are an expert in \"Reverse Prompt Engineering\". You are able to reverse-engineer prompts used to rewrite text. \n",
    "\n",
    "I will be providing you with an \"original text\" and \"rewritten text\". Please try to be as specific as possible and come up with a prompt that is based on the tone, style, and any other properties you consider relevant.\"\"\"\n",
    "\n",
    "def format_prompts(x):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"{sys_prompt}\\n{orig_prefix} {x['original_text']}\\n{rewrite_prefix} {x['rewritten_text']}\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"{response_start} {x['rewrite_prompt']}\"}\n",
    "    ]\n",
    "    output = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = False)\n",
    "    return {\"texts\": output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972f2bea5c5444148cbd4c6038413887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6856 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pl.read_csv('./data/predictions/combined-filtered_*.csv', ignore_errors=True)\n",
    "dataset = Dataset.from_list(df.to_dicts())\n",
    "dataset = dataset.map(format_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab9a5f0b55c46fea5cee0dbedc91c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/6856 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"texts\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 10,\n",
    "        # max_steps = 60,\n",
    "        num_train_epochs=3,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=200,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        # optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"./outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 6,856 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 2,571\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e485950f3e4e2ba2a1ca3af1b5cedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2628, 'grad_norm': 0.579164445400238, 'learning_rate': 0.00018516204607575167, 'epoch': 0.23}\n",
      "{'loss': 1.1619, 'grad_norm': 0.4446599781513214, 'learning_rate': 0.00016954314720812183, 'epoch': 0.47}\n",
      "{'loss': 1.1565, 'grad_norm': 0.44324904680252075, 'learning_rate': 0.000153924248340492, 'epoch': 0.7}\n",
      "{'loss': 1.1464, 'grad_norm': 0.44189155101776123, 'learning_rate': 0.0001383053494728622, 'epoch': 0.93}\n",
      "{'loss': 1.014, 'grad_norm': 0.52754807472229, 'learning_rate': 0.00012268645060523235, 'epoch': 1.17}\n",
      "{'loss': 0.9399, 'grad_norm': 0.6610332131385803, 'learning_rate': 0.0001070675517376025, 'epoch': 1.4}\n",
      "{'loss': 0.9322, 'grad_norm': 0.6497707962989807, 'learning_rate': 9.144865286997268e-05, 'epoch': 1.63}\n",
      "{'loss': 0.9279, 'grad_norm': 0.6296718120574951, 'learning_rate': 7.582975400234283e-05, 'epoch': 1.87}\n",
      "{'loss': 0.8416, 'grad_norm': 0.7901496887207031, 'learning_rate': 6.021085513471301e-05, 'epoch': 2.1}\n",
      "{'loss': 0.6892, 'grad_norm': 0.8700945973396301, 'learning_rate': 4.459195626708317e-05, 'epoch': 2.33}\n",
      "{'loss': 0.6987, 'grad_norm': 0.8211750984191895, 'learning_rate': 2.897305739945334e-05, 'epoch': 2.57}\n",
      "{'loss': 0.6913, 'grad_norm': 0.8318932056427002, 'learning_rate': 1.3354158531823507e-05, 'epoch': 2.8}\n",
      "{'train_runtime': 17743.3414, 'train_samples_per_second': 1.159, 'train_steps_per_second': 0.145, 'train_loss': 0.9367415206713325, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('/home/lawrence/Projects/my_models/mistral_pr_lora')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
