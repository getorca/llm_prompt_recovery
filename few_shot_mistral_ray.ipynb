{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from jinja2 import Environment, Template, meta\n",
    "import sys\n",
    "from itertools import product\n",
    "from datasets import load_dataset\n",
    "import polars as pl\n",
    "from vllm import LLM, SamplingParams\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "import ray\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Any, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "model_name = '/home/lawrence/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873'\n",
    "# model_name = 'google/gemma-7b-it'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the dataset into ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_input(row: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \n",
    "    #original text prefix\n",
    "    orig_prefix = \"Original Text:\"\n",
    "    #modified text prefix\n",
    "    rewrite_prefix = \"Rewritten Text:\"\n",
    "    # response start\n",
    "    response_start = \"The prompt was:\"\n",
    "\n",
    "    sys_prompt = \"\"\"You are an expert in \"Reverse Prompt Engineering\". You are able to reverse-engineer prompts used to rewrite text.\\n\\nI will be providing you with an \"original text\" and \"rewritten text\". Please try to be as specific as possible and come up with a prompt that is based on the tone, style, and any other properties you consider relevant.\"\"\"\n",
    "\n",
    "    #first example\n",
    "    ex_text_1 = \"The aroma of freshly baked bread wafted through the bustling marketplace, mingling with the shouts of vendors and the excited chatter of customers. A lone scout, her cloak dusted with desert sand, weaved through the crowd, her hand instinctively resting on the hilt of her dagger.\"\n",
    "    ex_rewrite_1 = \"A scent of opportunity fills the air. Trust your instincts and a hidden path will reveal itself.\"\n",
    "    ex_prompt_1 = \"Craft this as a fortune cookie message\"\n",
    "\n",
    "    #second example\n",
    "    ex_text_2 = \"The old lighthouse keeper shuffled down the rickety stairs, his pockets jangling with forgotten keys. He peered out at the churning sea, the salty spray stinging his weathered face. A lone seagull circled overhead, its mournful cry echoing in the wind.\"\n",
    "    ex_rewrite_2 = \"The lighthouse walls creaked with each of the keeper's slow steps, the rhythm a familiar lullaby to the young apprentice perched by the window. He watched the old man descend, a wizened silhouette framed by the churning sea. The salty spray, invisible from his vantage point, seemed to dance on the keeper's face, etching new lines into the weathered map. The cry of the lone seagull was a melody only the old man understood, a language that spoke of years spent guarding this solitary beacon.\"\n",
    "    ex_prompt_2 = \"Reimagine this scene from a different perspective, offering a fresh insight into the events\"\n",
    "\n",
    "    #second example\n",
    "    ex_text_3 = \"The population of the earth is estimated to be around 8 billion people, but there are far more insects! Scientists believe that there are over 10 quintillion (that's 10, followed by 18 zeros) individual insects alive at any given time, outweighing all humans and animals combined by a staggering amount.\"\n",
    "    ex_rewrite_3 = \"Wow, have you ever stopped to think about just how many insects there are? Our planet may be teeming with 8 billion people, but that's nothing compared to the incredible number of insects buzzing, crawling, and flitting around us. Scientists estimate there are over 10 quintillion, that's a 10 with 18 zeros, individual insects alive at any given moment! That's mind-boggling!  And get this, all those insects together weigh more than all the humans and animals on Earth combined.  Pretty incredible, right?\"\n",
    "    ex_prompt_3 = \"Rewrite the following as a speech\"\n",
    "\n",
    "    messages = [\n",
    "        #first example\n",
    "        {\"role\": \"user\", \"content\": f\"{sys_prompt}\\n{orig_prefix} {ex_text_1}\\n{rewrite_prefix} {ex_rewrite_1}\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"{response_start} {ex_prompt_1}\"},\n",
    "\n",
    "        #second example\n",
    "        {\"role\": \"user\", \"content\": f\"{orig_prefix} {ex_text_2}\\n{rewrite_prefix} {ex_rewrite_2}\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"{response_start} {ex_prompt_2}\"},\n",
    "\n",
    "        #actual prompt\n",
    "        {\"role\": \"user\", \"content\": f\"{orig_prefix}{ex_text_3}\\n{rewrite_prefix} {ex_rewrite_3}\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"{response_start} {ex_prompt_3}\"},\n",
    "        \n",
    "        #actual prompt\n",
    "        {\"role\": \"user\", \"content\": f\"{orig_prefix} {row['original_text']}\\n{rewrite_prefix} {row['rewritten_text']}\"},\n",
    "        {\"role\": \"assistant\", \"content\": response_start},\n",
    "    ]\n",
    "        \n",
    "    #give it to Mistral\n",
    "    row['input'] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    \n",
    "    return row\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# ds = (\n",
    "#     ray.data.read_csv('data/3rd_party_ds/llm_dataset_1.csv')\n",
    "#     .map(get_chat_input)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage stats collection is enabled by default for nightly wheels. To disable this, run the following command: `ray disable-usage-stats` before starting Ray. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-06 22:03:01,258\tINFO worker.py:1752 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/lawrence/Projects/contests/llm_prompt_recovery/data/3rd_party_ds/gemma-rewrite-nbroad/nbroad-v2.csv')\n",
    "df.rename(columns={'rewrite_prompt': 'gt_rewrite_prompt'}, inplace=True)\n",
    "ds = (\n",
    "    ray.data.from_pandas(df)\n",
    "    .map(get_chat_input)\n",
    ").repartition(2) # need to force into multiple blocks <https://discuss.ray.io/t/single-node-4x-gpu-map-batches-only-using-1/12313/2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>gt_rewrite_prompt</th>\n",
       "      <th>rewritten_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lZGdiueMer</td>\n",
       "      <td>`` Well, there are healthier ways to tell me y...</td>\n",
       "      <td>Rewrite the story where the writer asks the re...</td>\n",
       "      <td>Well, there are healthier ways to tell me you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DfTJVFKrUk</td>\n",
       "      <td>Rory ran his shaky fingers through his wife's ...</td>\n",
       "      <td>Rewrite the essay as a dramatic play</td>\n",
       "      <td>## The Final Curtain\\n\\n[FADE IN]\\n\\n**Setting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LmJvKranXK</td>\n",
       "      <td>As I made my way on foot across town to the Po...</td>\n",
       "      <td>Rewrite the story with all the themes and sett...</td>\n",
       "      <td>As I made my way through the Tatooine desert o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PpnqXQAdGH</td>\n",
       "      <td>`` Hello. We come in peace.'' \\n \\n The first ...</td>\n",
       "      <td>Rewrite the essay if the advanced aliens didn'...</td>\n",
       "      <td>`` Hello. We come in peace.''\\n\\nThe first enc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qOeXTfqgAM</td>\n",
       "      <td>`` Karen, what the helllllll izzz...'' says my...</td>\n",
       "      <td>Rewrite the story as a court room drama starri...</td>\n",
       "      <td>The courtroom erupted in an uproar as District...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2395</th>\n",
       "      <td>ljSaWTXjFc</td>\n",
       "      <td>`` Dave, have you seen this?'' There is no ans...</td>\n",
       "      <td>Rewrite this as a Shakespearean tragedy</td>\n",
       "      <td>Alas, poor Dave, a victim of fate's cruel hand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2396</th>\n",
       "      <td>nwZeXhsofO</td>\n",
       "      <td>I posted something like this a while ago, I gu...</td>\n",
       "      <td>Rewrite the story as if you are a famous celeb...</td>\n",
       "      <td>I'm the epitome of stardom, yet I can't shake ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>ozaryEVFvK</td>\n",
       "      <td>`` Oh Darling, it was absolutely dreadful!'' M...</td>\n",
       "      <td>Rewrite the story as an upbeat mystery story (...</td>\n",
       "      <td>Hey folks, buckle up for a mystery that will h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>WTvBtRNlQS</td>\n",
       "      <td>Ubwuwuaua! I exclaim shaking my head back and ...</td>\n",
       "      <td>Rewrite the story from the perspective of the ...</td>\n",
       "      <td>Ubwuwuaua. The whispers of the wind dance thro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>jcLSWkFNKS</td>\n",
       "      <td>`` Yeah I'll hook you up fam, how many?'' \\n \\...</td>\n",
       "      <td>Rewrite this story to focus on the moral dilem...</td>\n",
       "      <td>The aroma of fresh pizza filled the air, but t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2400 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                      original_text  \\\n",
       "0     lZGdiueMer  `` Well, there are healthier ways to tell me y...   \n",
       "1     DfTJVFKrUk  Rory ran his shaky fingers through his wife's ...   \n",
       "2     LmJvKranXK  As I made my way on foot across town to the Po...   \n",
       "3     PpnqXQAdGH  `` Hello. We come in peace.'' \\n \\n The first ...   \n",
       "4     qOeXTfqgAM  `` Karen, what the helllllll izzz...'' says my...   \n",
       "...          ...                                                ...   \n",
       "2395  ljSaWTXjFc  `` Dave, have you seen this?'' There is no ans...   \n",
       "2396  nwZeXhsofO  I posted something like this a while ago, I gu...   \n",
       "2397  ozaryEVFvK  `` Oh Darling, it was absolutely dreadful!'' M...   \n",
       "2398  WTvBtRNlQS  Ubwuwuaua! I exclaim shaking my head back and ...   \n",
       "2399  jcLSWkFNKS  `` Yeah I'll hook you up fam, how many?'' \\n \\...   \n",
       "\n",
       "                                      gt_rewrite_prompt  \\\n",
       "0     Rewrite the story where the writer asks the re...   \n",
       "1                  Rewrite the essay as a dramatic play   \n",
       "2     Rewrite the story with all the themes and sett...   \n",
       "3     Rewrite the essay if the advanced aliens didn'...   \n",
       "4     Rewrite the story as a court room drama starri...   \n",
       "...                                                 ...   \n",
       "2395            Rewrite this as a Shakespearean tragedy   \n",
       "2396  Rewrite the story as if you are a famous celeb...   \n",
       "2397  Rewrite the story as an upbeat mystery story (...   \n",
       "2398  Rewrite the story from the perspective of the ...   \n",
       "2399  Rewrite this story to focus on the moral dilem...   \n",
       "\n",
       "                                         rewritten_text  \n",
       "0     Well, there are healthier ways to tell me you ...  \n",
       "1     ## The Final Curtain\\n\\n[FADE IN]\\n\\n**Setting...  \n",
       "2     As I made my way through the Tatooine desert o...  \n",
       "3     `` Hello. We come in peace.''\\n\\nThe first enc...  \n",
       "4     The courtroom erupted in an uproar as District...  \n",
       "...                                                 ...  \n",
       "2395  Alas, poor Dave, a victim of fate's cruel hand...  \n",
       "2396  I'm the epitome of stardom, yet I can't shake ...  \n",
       "2397  Hey folks, buckle up for a mystery that will h...  \n",
       "2398  Ubwuwuaua. The whispers of the wind dance thro...  \n",
       "2399  The aroma of fresh pizza filled the air, but t...  \n",
       "\n",
       "[2400 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sampling params object.\n",
    "sampling_params = SamplingParams(max_tokens=100)\n",
    "\n",
    "\n",
    "# Create a class to do batch inference.\n",
    "class LLMPredictor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.llm = LLM(\n",
    "            model=model_name,\n",
    "            gpu_memory_utilization=0.92,\n",
    "            max_model_len=1500,\n",
    "            trust_remote_code=True, \n",
    "        )\n",
    "\n",
    "    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, list]:\n",
    "        outputs = self.llm.generate(batch[\"input\"], sampling_params)\n",
    "        \n",
    "        generated_text = []\n",
    "        for output in outputs:\n",
    "            generated_text.append(' '.join([o.text for o in output.outputs]))\n",
    "        batch[\"rewrite_prompt\"] = generated_text\n",
    "\n",
    "        return batch\n",
    "\n",
    "ds = ds.map_batches(\n",
    "    LLMPredictor,\n",
    "    # Set the concurrency to the number of LLM instances.\n",
    "    concurrency=2,\n",
    "    # Specify the number of GPUs required per LLM instance.\n",
    "    # NOTE: Do NOT set `num_gpus` when using vLLM with tensor-parallelism\n",
    "    # (i.e., `tensor_parallel_size`).\n",
    "    num_gpus=1,\n",
    "    # Specify the batch size for inference.\n",
    "    batch_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-06 22:03:04,031\tINFO streaming_executor.py:115 -- Starting execution of Dataset. Full log is in /tmp/ray/session_2024-04-06_22-02-59_228988_439769/logs/ray-data.log\n",
      "2024-04-06 22:03:04,032\tINFO streaming_executor.py:116 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[Map(get_chat_input)] -> AllToAllOperator[Repartition] -> ActorPoolMapOperator[MapBatches(LLMPredictor)] -> TaskPoolMapOperator[Write]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=441463)\u001b[0m INFO 04-06 22:03:07 llm_engine.py:87] Initializing an LLM engine with config: model='/home/lawrence/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873', tokenizer='/home/lawrence/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=1500, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "\u001b[36m(_MapWorker pid=441463)\u001b[0m INFO 04-06 22:03:15 llm_engine.py:357] # GPU blocks: 3727, # CPU blocks: 2048\n",
      "\u001b[36m(_MapWorker pid=441462)\u001b[0m INFO 04-06 22:03:07 llm_engine.py:87] Initializing an LLM engine with config: model='/home/lawrence/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873', tokenizer='/home/lawrence/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=1500, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "\u001b[36m(_MapWorker pid=441463)\u001b[0m INFO 04-06 22:03:17 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(_MapWorker pid=441463)\u001b[0m INFO 04-06 22:03:17 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(_MapWorker pid=441463)\u001b[0m INFO 04-06 22:03:22 model_runner.py:756] Graph capturing finished in 5 secs.\n",
      "\u001b[36m(_MapWorker pid=441462)\u001b[0m INFO 04-06 22:03:24 llm_engine.py:357] # GPU blocks: 3728, # CPU blocks: 2048\n",
      "\u001b[36m(_MapWorker pid=441462)\u001b[0m INFO 04-06 22:03:25 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(_MapWorker pid=441462)\u001b[0m INFO 04-06 22:03:25 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(_MapWorker pid=441462)\u001b[0m INFO 04-06 22:03:31 model_runner.py:756] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a8829cb2144028a9309c679ed71d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Map(get_chat_input) 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16de973d14874b85ae244029893be714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Repartition 2:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e28b4a4cbb74d8ebbb435d7c243dc26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split Repartition 3:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7ff25521af4ced90d64627e54b43db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(LLMPredictor) 4:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749130cf233d4b808330df9e1294fe77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Write 5:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5a8e9ca9c944408418389a80a17550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:03:37 scheduler.py:195] Input prompt (2948 tokens) is too long and exceeds limit of 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.84it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.23s/it]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8648.05it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8081.51it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9489.38it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8952.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:03:42 scheduler.py:195] Input prompt (2114 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 38x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.84it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9467.95it/s]\u001b[32m [repeated 18x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:03:47 scheduler.py:195] Input prompt (1635 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 43x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.87it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.73it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8603.70it/s]\u001b[32m [repeated 20x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:03:53 scheduler.py:195] Input prompt (1689 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 17x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.67it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it]\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.67it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:04:00 scheduler.py:195] Input prompt (3222 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8004.40it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.34s/it]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8551.08it/s]\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.86it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:04:05 scheduler.py:195] Input prompt (2049 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 12x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9049.20it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.27it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8971.77it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:04:11 scheduler.py:195] Input prompt (1745 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 20x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.89it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9127.97it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:04:16 scheduler.py:195] Input prompt (3687 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 18x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:04:22 scheduler.py:195] Input prompt (3232 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 19x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:02<00:02,  2.21s/it]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 7564.12it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.44s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.85it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 10046.24it/s]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:04:27 scheduler.py:195] Input prompt (3169 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 24x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.29it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9788.34it/s]\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:04:32 scheduler.py:195] Input prompt (1692 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 39x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:01<00:01,  1.36s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8783.88it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:04:38 scheduler.py:195] Input prompt (2463 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 17x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.44it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8297.34it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.83it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:04:44 scheduler.py:195] Input prompt (2074 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 20x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.84it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9157.87it/s]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.23s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:04:49 scheduler.py:195] Input prompt (2205 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 28x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:01<00:01,  1.26s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8525.01it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:04:55 scheduler.py:195] Input prompt (2804 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 20x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.68it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 10155.70it/s]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:05:01 scheduler.py:195] Input prompt (3090 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 18x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.54it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9720.29it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.25s/it]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:05:06 scheduler.py:195] Input prompt (2843 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 28x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.86it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8104.94it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.57it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:05:12 scheduler.py:195] Input prompt (1710 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.64it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 6232.25it/s]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.25s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:05:18 scheduler.py:195] Input prompt (1568 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 22x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.26it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8224.13it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:05:24 scheduler.py:195] Input prompt (1616 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.28it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8322.03it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.55it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:05:29 scheduler.py:195] Input prompt (2022 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 15x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.13it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8322.03it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.70it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:05:35 scheduler.py:195] Input prompt (1895 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 20x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.66it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 10565.00it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:05:41 scheduler.py:195] Input prompt (2455 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 14x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:02<00:02,  2.13s/it]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8397.01it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.22s/it]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9404.27it/s]\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  1.15it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9686.61it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:05:48 scheduler.py:195] Input prompt (1978 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 24x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.52it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 11096.04it/s]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.26s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:05:53 scheduler.py:195] Input prompt (1545 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 13x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:01<00:01,  1.32s/it]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9414.82it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:05:59 scheduler.py:195] Input prompt (2818 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:06:05 scheduler.py:195] Input prompt (1872 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.57it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9788.34it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8710.91it/s]\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.56it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.09it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:06:10 scheduler.py:195] Input prompt (2117 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 20x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9269.18it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:01<00:01,  1.64s/it]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.21s/it]\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:06:16 scheduler.py:195] Input prompt (1876 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 13x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8081.51it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.80it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.26s/it]\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:06:21 scheduler.py:195] Input prompt (2000 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 12x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9310.33it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:06:27 scheduler.py:195] Input prompt (2594 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 36x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.36it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.22s/it]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8943.08it/s]\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:06:33 scheduler.py:195] Input prompt (1964 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 12x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.42it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.21s/it]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 7463.17it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:06:38 scheduler.py:195] Input prompt (1618 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 9x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.46s/it]\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.55it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.27s/it]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9675.44it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.30it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.22s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 7084.97it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:06:45 scheduler.py:195] Input prompt (3210 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 18x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.81it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.32s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 6497.76it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:06:52 scheduler.py:195] Input prompt (1568 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 21x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:01<00:01,  1.20s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.05it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9446.63it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:06:57 scheduler.py:195] Input prompt (1814 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 19x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.32it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.27s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8289.14it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:07:02 scheduler.py:195] Input prompt (2808 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 13x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.98it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.77it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9208.13it/s]\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:07:08 scheduler.py:195] Input prompt (1897 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 9x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.49it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.50it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9157.87it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:07:14 scheduler.py:195] Input prompt (1934 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 26x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.46it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8683.86it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:07:20 scheduler.py:195] Input prompt (1870 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 21x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.32it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.59it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8586.09it/s]\u001b[32m [repeated 11x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:07:25 scheduler.py:195] Input prompt (1791 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 12x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.84it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.87it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9927.35it/s]\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:07:31 scheduler.py:195] Input prompt (1701 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 28x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.19it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.22s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8516.35it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:07:36 scheduler.py:195] Input prompt (2954 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 14x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it]\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.06it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.27s/it]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8525.01it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:07:41 scheduler.py:195] Input prompt (3270 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 17x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.29it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8962.19it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:07:46 scheduler.py:195] Input prompt (1602 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.42s/it]\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.30it/s]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.26s/it]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8674.88it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:07:52 scheduler.py:195] Input prompt (2178 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 15x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.80it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.32s/it]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8811.56it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:07:57 scheduler.py:195] Input prompt (2544 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 15x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:02<00:02,  2.74s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  3.24it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8355.19it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:08:03 scheduler.py:195] Input prompt (1670 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 24x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.89it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8363.52it/s]\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:08:09 scheduler.py:195] Input prompt (3436 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 14x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.83it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  3.06it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8830.11it/s]\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:08:16 scheduler.py:195] Input prompt (1743 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 32x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:01<00:01,  1.37s/it]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8876.83it/s]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9068.77it/s]\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.63it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.21s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8338.58it/s]\u001b[32m [repeated 14x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:08:21 scheduler.py:195] Input prompt (2343 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 36x across cluster]\u001b[0m\n",
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:08:27 scheduler.py:195] Input prompt (1561 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 22x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.44it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.21it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8322.03it/s]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.28it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 5265.92it/s]\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:08:34 scheduler.py:195] Input prompt (1925 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 21x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.66it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.81it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8971.77it/s]\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:08:40 scheduler.py:195] Input prompt (1807 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 28x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.21it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.27s/it]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8490.49it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:08:45 scheduler.py:195] Input prompt (2318 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 17x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:01<00:01,  1.95s/it]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.31s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9446.63it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:08:50 scheduler.py:195] Input prompt (2035 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 12x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.30it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.26s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8289.14it/s]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:08:55 scheduler.py:195] Input prompt (2626 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 26x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  1.90it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  3.33it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8035.07it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:09:02 scheduler.py:195] Input prompt (1641 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 14x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.02it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8630.26it/s]\u001b[32m [repeated 13x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:09:08 scheduler.py:195] Input prompt (1770 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 33x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  1.94it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8208.03it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:09:14 scheduler.py:195] Input prompt (1707 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 28x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  1.08it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.81it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 10551.71it/s]\u001b[32m [repeated 12x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:09:19 scheduler.py:195] Input prompt (1877 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 18x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.29it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.31s/it]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8568.55it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.38it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.18s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8136.38it/s]\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:09:27 scheduler.py:195] Input prompt (1673 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 11x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.43s/it]\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.38it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.86it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8848.74it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:09:34 scheduler.py:195] Input prompt (2373 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 24x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.50it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.40s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8112.77it/s]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:09:39 scheduler.py:195] Input prompt (2494 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 21x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:01<00:01,  1.74s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 10058.28it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:09:46 scheduler.py:195] Input prompt (2083 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 24x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.35it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.22s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9608.94it/s]\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:09:52 scheduler.py:195] Input prompt (1768 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 22x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.48it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.49s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 7796.10it/s]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:02<00:02,  2.35s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9269.18it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:09:58 scheduler.py:195] Input prompt (2485 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 22x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.35it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.32s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9039.45it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:10:04 scheduler.py:195] Input prompt (1566 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 16x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:10:10 scheduler.py:195] Input prompt (1902 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 17x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.31it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.21s/it]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9049.20it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.84it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.48s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:10:16 scheduler.py:195] Input prompt (2493 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8747.25it/s]\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:01<00:01,  1.65s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.32s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:10:22 scheduler.py:195] Input prompt (2312 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 21x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8447.74it/s]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.84it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.33s/it]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8876.83it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:10:28 scheduler.py:195] Input prompt (1704 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 13x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.84it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9383.23it/s]\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:10:34 scheduler.py:195] Input prompt (2082 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 19x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.57it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.11it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8473.34it/s]\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:10:39 scheduler.py:195] Input prompt (1990 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:01<00:01,  1.21s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.78it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.40s/it]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:10:45 scheduler.py:195] Input prompt (2181 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 11x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 6302.49it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.85it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:10:50 scheduler.py:195] Input prompt (2304 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 20x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8710.91it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.90it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:10:56 scheduler.py:195] Input prompt (1917 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 17x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9039.45it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.18it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:11:02 scheduler.py:195] Input prompt (1847 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 22x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9218.25it/s]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.30it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.08it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8363.52it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:11:07 scheduler.py:195] Input prompt (2628 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 16x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.54it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.22s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8322.03it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:11:12 scheduler.py:195] Input prompt (2432 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 12x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  4.70it/s]\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:01<00:01,  1.48s/it]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.40s/it]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8577.31it/s]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8120.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:11:18 scheduler.py:195] Input prompt (2232 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 21x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:02<00:02,  2.12s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.21s/it]\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:11:23 scheduler.py:195] Input prompt (2008 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 11x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8568.55it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9478.65it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9187.96it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9320.68it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8603.70it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 7810.62it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9597.95it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8413.85it/s]\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.23it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.21s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 7854.50it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8783.88it/s]\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:11:28 scheduler.py:195] Input prompt (1842 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 22x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.50it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8152.19it/s]\u001b[32m [repeated 9x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:11:35 scheduler.py:195] Input prompt (1561 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 18x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:02<00:02,  2.40s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.44s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 4578.93it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:11:40 scheduler.py:195] Input prompt (1931 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 14x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.73it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.00it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 7876.63it/s]\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:11:46 scheduler.py:195] Input prompt (1921 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.89it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 7884.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:11:51 scheduler.py:195] Input prompt (2507 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8656.97it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9020.01it/s]\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.30it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:11:58 scheduler.py:195] Input prompt (3271 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 15x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9187.96it/s]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.33it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.35s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9279.43it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:12:03 scheduler.py:195] Input prompt (1758 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 18x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.35it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.33s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.45s/it]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8397.01it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:02<00:02,  2.55s/it]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.24it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:12:10 scheduler.py:195] Input prompt (2343 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 20x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 7307.15it/s]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.37it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.25s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:12:16 scheduler.py:195] Input prompt (1796 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 34x across cluster]\u001b[0m\n",
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:12:21 scheduler.py:195] Input prompt (3878 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8568.55it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.29it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  2.01it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 10106.76it/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9010.32it/s]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.68it/s]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:12:26 scheduler.py:195] Input prompt (1524 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 30x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 9078.58it/s]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.15it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it]\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m WARNING 04-06 22:12:32 scheduler.py:195] Input prompt (1820 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 24x across cluster]\u001b[0m\n",
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:12:37 scheduler.py:195] Input prompt (1547 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8430.76it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.56it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441463)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=441462)\u001b[0m WARNING 04-06 22:12:43 scheduler.py:195] Input prompt (2670 tokens) is too long and exceeds limit of 1500\u001b[32m [repeated 17x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00, 8371.86it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  3.40it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Processed prompts: 100%|██████████| 2/2 [00:02<00:00,  1.21s/it]\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ds.write_parquet(\"./data/exp_test/nboard_v2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pl.read_parquet(\"./data/exp_test/nboard_v2/*.parquet\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>gt_rewrite_prompt</th>\n",
       "      <th>rewritten_text</th>\n",
       "      <th>input</th>\n",
       "      <th>rewrite_prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KisiIFfldx</td>\n",
       "      <td>Listen, people seem to think that monsters are...</td>\n",
       "      <td>Rewrite the story as a lighthearted and funny ...</td>\n",
       "      <td>Have you ever been scared by a monster? Oh boy...</td>\n",
       "      <td>&lt;s&gt;[INST] You are an expert in \"Reverse Prompt...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UyqBvEQOkQ</td>\n",
       "      <td>Like stealing candy from a baby. Mika was the ...</td>\n",
       "      <td>Rewrite the story to have a plot twist halfway...</td>\n",
       "      <td>Like stealing candy from a baby. Mika was the ...</td>\n",
       "      <td>&lt;s&gt;[INST] You are an expert in \"Reverse Prompt...</td>\n",
       "      <td>Rewrite the text as if it's a confession or a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CwZGipQWvD</td>\n",
       "      <td>`` BILLY MAYS HERE!'' The fireplace burst into...</td>\n",
       "      <td>Rewrite the essay as a philosophical argument ...</td>\n",
       "      <td>The sales process inherently raises ethical qu...</td>\n",
       "      <td>&lt;s&gt;[INST] You are an expert in \"Reverse Prompt...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pKXubhhwgC</td>\n",
       "      <td>** [ This is my first writing here, and I am n...</td>\n",
       "      <td>Rewrite the story as a superhero comic from Ma...</td>\n",
       "      <td>**The Crimson Guardian**\\n\\nIn the heart of a ...</td>\n",
       "      <td>&lt;s&gt;[INST] You are an expert in \"Reverse Prompt...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IwboJVgxTC</td>\n",
       "      <td>`` I do n't like it, this things to hot to kee...</td>\n",
       "      <td>Rewrite the scene so that the hijackers are pr...</td>\n",
       "      <td>The bank was abuzz with life. People lined up,...</td>\n",
       "      <td>&lt;s&gt;[INST] You are an expert in \"Reverse Prompt...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2395</th>\n",
       "      <td>hQtnmdqLsZ</td>\n",
       "      <td>“ I ’ m telling you, the house has been empty ...</td>\n",
       "      <td>Rewrite the story from the perspective of the ...</td>\n",
       "      <td>The scent of bleach and damp earth lingered in...</td>\n",
       "      <td>&lt;s&gt;[INST] You are an expert in \"Reverse Prompt...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2396</th>\n",
       "      <td>xZSLwkEnEc</td>\n",
       "      <td>`` OW! WHAT THE FUCK IS WRONG WITH YOU?'' Jeni...</td>\n",
       "      <td>Rewrite it as a story about a monster who is f...</td>\n",
       "      <td>In the realm of terror and devastation, a mons...</td>\n",
       "      <td>&lt;s&gt;[INST] You are an expert in \"Reverse Prompt...</td>\n",
       "      <td>Transform the following text into a mythologi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>qlVmrrkVcL</td>\n",
       "      <td>The woman spoke first. `` Help me! He's with t...</td>\n",
       "      <td>Rewrite the essay as a murder mystery where yo...</td>\n",
       "      <td>A murder mystery unfolds as you read this narr...</td>\n",
       "      <td>&lt;s&gt;[INST] You are an expert in \"Reverse Prompt...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>EbBxajyzHq</td>\n",
       "      <td>Chijin hastened his team again. The barricade ...</td>\n",
       "      <td>Rewrite the essay in the style of an episode o...</td>\n",
       "      <td>\"The Bullpen Bar was buzzing with the scent of...</td>\n",
       "      <td>&lt;s&gt;[INST] You are an expert in \"Reverse Prompt...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>AEUbcatzsb</td>\n",
       "      <td>`` So, this is your first day then?'' \\n \\n Ba...</td>\n",
       "      <td>Rewrite the story by showing the day in the li...</td>\n",
       "      <td>So, this is your first day then?''\\n\\nMaria sa...</td>\n",
       "      <td>&lt;s&gt;[INST] You are an expert in \"Reverse Prompt...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2400 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                      original_text  \\\n",
       "0     KisiIFfldx  Listen, people seem to think that monsters are...   \n",
       "1     UyqBvEQOkQ  Like stealing candy from a baby. Mika was the ...   \n",
       "2     CwZGipQWvD  `` BILLY MAYS HERE!'' The fireplace burst into...   \n",
       "3     pKXubhhwgC  ** [ This is my first writing here, and I am n...   \n",
       "4     IwboJVgxTC  `` I do n't like it, this things to hot to kee...   \n",
       "...          ...                                                ...   \n",
       "2395  hQtnmdqLsZ  “ I ’ m telling you, the house has been empty ...   \n",
       "2396  xZSLwkEnEc  `` OW! WHAT THE FUCK IS WRONG WITH YOU?'' Jeni...   \n",
       "2397  qlVmrrkVcL  The woman spoke first. `` Help me! He's with t...   \n",
       "2398  EbBxajyzHq  Chijin hastened his team again. The barricade ...   \n",
       "2399  AEUbcatzsb  `` So, this is your first day then?'' \\n \\n Ba...   \n",
       "\n",
       "                                      gt_rewrite_prompt  \\\n",
       "0     Rewrite the story as a lighthearted and funny ...   \n",
       "1     Rewrite the story to have a plot twist halfway...   \n",
       "2     Rewrite the essay as a philosophical argument ...   \n",
       "3     Rewrite the story as a superhero comic from Ma...   \n",
       "4     Rewrite the scene so that the hijackers are pr...   \n",
       "...                                                 ...   \n",
       "2395  Rewrite the story from the perspective of the ...   \n",
       "2396  Rewrite it as a story about a monster who is f...   \n",
       "2397  Rewrite the essay as a murder mystery where yo...   \n",
       "2398  Rewrite the essay in the style of an episode o...   \n",
       "2399  Rewrite the story by showing the day in the li...   \n",
       "\n",
       "                                         rewritten_text  \\\n",
       "0     Have you ever been scared by a monster? Oh boy...   \n",
       "1     Like stealing candy from a baby. Mika was the ...   \n",
       "2     The sales process inherently raises ethical qu...   \n",
       "3     **The Crimson Guardian**\\n\\nIn the heart of a ...   \n",
       "4     The bank was abuzz with life. People lined up,...   \n",
       "...                                                 ...   \n",
       "2395  The scent of bleach and damp earth lingered in...   \n",
       "2396  In the realm of terror and devastation, a mons...   \n",
       "2397  A murder mystery unfolds as you read this narr...   \n",
       "2398  \"The Bullpen Bar was buzzing with the scent of...   \n",
       "2399  So, this is your first day then?''\\n\\nMaria sa...   \n",
       "\n",
       "                                                  input  \\\n",
       "0     <s>[INST] You are an expert in \"Reverse Prompt...   \n",
       "1     <s>[INST] You are an expert in \"Reverse Prompt...   \n",
       "2     <s>[INST] You are an expert in \"Reverse Prompt...   \n",
       "3     <s>[INST] You are an expert in \"Reverse Prompt...   \n",
       "4     <s>[INST] You are an expert in \"Reverse Prompt...   \n",
       "...                                                 ...   \n",
       "2395  <s>[INST] You are an expert in \"Reverse Prompt...   \n",
       "2396  <s>[INST] You are an expert in \"Reverse Prompt...   \n",
       "2397  <s>[INST] You are an expert in \"Reverse Prompt...   \n",
       "2398  <s>[INST] You are an expert in \"Reverse Prompt...   \n",
       "2399  <s>[INST] You are an expert in \"Reverse Prompt...   \n",
       "\n",
       "                                         rewrite_prompt  \n",
       "0                                                        \n",
       "1      Rewrite the text as if it's a confession or a...  \n",
       "2                                                        \n",
       "3                                                        \n",
       "4                                                        \n",
       "...                                                 ...  \n",
       "2395                                                     \n",
       "2396   Transform the following text into a mythologi...  \n",
       "2397                                                     \n",
       "2398                                                     \n",
       "2399                                                     \n",
       "\n",
       "[2400 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## score the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48421794"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "st_model = SentenceTransformer('sentence-transformers/sentence-t5-base')\n",
    "\n",
    "def get_sharpened_cosine_similarity(text1, text2):\n",
    "    embeddings1 = st_model.encode(text1)\n",
    "    embeddings2 = st_model.encode(text2)\n",
    "    cosine_score = util.cos_sim(embeddings1, embeddings2)\n",
    "    # print(cosine_score) \n",
    "    return (cosine_score[0] ** 3).numpy()[0]\n",
    "\n",
    "def calc_prompt_similarity(row):\n",
    "    return get_sharpened_cosine_similarity(row['gt_rewrite_prompt'], row['rewrite_prompt'])\n",
    "\n",
    "train_df['score'] = train_df.apply(lambda row: calc_prompt_similarity(row), axis=1)\n",
    "\n",
    "train_df['score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('./data/predictions/nboard_v2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_promt_reversal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
