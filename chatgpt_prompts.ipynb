{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from jinja2 import Environment, Template, meta\n",
    "import sys\n",
    "from itertools import product\n",
    "from datasets import load_dataset\n",
    "import polars as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv('data/exp_6/wiki_movie_plots_deduped.csv').with_columns([\n",
    "    pl.col('Plot').alias('original_text'),\n",
    "]).select(\n",
    "    pl.col('original_text').shuffle(seed=6541)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (20, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>original_text</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;The Lion Has W…</td></tr><tr><td>&quot;San Francisco …</td></tr><tr><td>&quot;As described i…</td></tr><tr><td>&quot;Rashad is a te…</td></tr><tr><td>&quot;Mary Kirk Loga…</td></tr><tr><td>&hellip;</td></tr><tr><td>&quot;Mr. Tucker (Pl…</td></tr><tr><td>&quot;During the Pro…</td></tr><tr><td>&quot;He Dashang (Wu…</td></tr><tr><td>&quot;Kavitha (Sujat…</td></tr><tr><td>&quot;The film opens…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (20, 1)\n",
       "┌───────────────────────────────────┐\n",
       "│ original_text                     │\n",
       "│ ---                               │\n",
       "│ str                               │\n",
       "╞═══════════════════════════════════╡\n",
       "│ The Lion Has Wings is recounted … │\n",
       "│ San Francisco Assistant District… │\n",
       "│ As described in a film magazine,… │\n",
       "│ Rashad is a teen living in Atlan… │\n",
       "│ Mary Kirk Logan is led from her … │\n",
       "│ …                                 │\n",
       "│ Mr. Tucker (Platt), proprietor o… │\n",
       "│ During the Prohibition era, a yo… │\n",
       "│ He Dashang (Wu Gang) is a middle… │\n",
       "│ Kavitha (Sujatha) is a working w… │\n",
       "│ The film opens on an Egyptian ar… │\n",
       "└───────────────────────────────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[:20]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built from `https://www.kaggle.com/datasets/ilanmeissonnier/chatgpt-rewrite-promts/data` & `https://www.kaggle.com/datasets/richolson/600-gpt4-re-write-prompts`\n",
    "\n",
    "prompts_df = pl.read_csv('./data/exp_6/prompts/prompts.csv').select(\n",
    "    pl.col('rewrite_prompt').alias('prompt')\n",
    ").extend(\n",
    "    pl.read_csv('./data/exp_6/prompts/gpt4_prompts.csv')\n",
    ").extend(\n",
    "    pl.read_csv('./data/3rd_party_ds/rewritten_texts_csv_v3.csv', ignore_errors=True).select('prompt').unique()\n",
    ")\n",
    "\n",
    "prompts = prompts_df['prompt'].unique().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage stats collection is enabled by default for nightly wheels. To disable this, run the following command: `ray disable-usage-stats` before starting Ray. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-05 23:12:51,974\tINFO worker.py:1752 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "\n",
    "prompt_ds = []\n",
    "\n",
    "idx = 0\n",
    "for row in df.iter_rows(named=True):\n",
    "    prompt_ds.append({\n",
    "        'original_text': row['original_text'],\n",
    "        'prompt': prompts[idx], \n",
    "        'input': f'''<start_of_turn>user\n",
    "{prompts[idx]}: \"\"\"{row[\"original_text\"]}\"\"\"<end_of_turn>\n",
    "<start_of_turn>model\n",
    "'''\n",
    "    })\n",
    "    idx = idx + 1 if idx < (len(prompts)-1) else 0\n",
    "    \n",
    "    \n",
    "# prompt_set\n",
    "\n",
    "ds = ray.data.from_items(prompt_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from typing import Dict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sampling params object.\n",
    "sampling_params = SamplingParams(max_tokens=200)\n",
    "\n",
    "\n",
    "# Create a class to do batch inference.\n",
    "class LLMPredictor:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Create an LLM.\n",
    "        self.llm = LLM(\n",
    "            model=\"google/gemma-7b-it\",\n",
    "            gpu_memory_utilization=0.95,\n",
    "            max_model_len=1500,    \n",
    "        )\n",
    "\n",
    "    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, list]:\n",
    "        # Generate texts from the prompts.\n",
    "        # The output is a list of RequestOutput objects that contain the prompt,\n",
    "        # generated text, and other information.\n",
    "        # prompt = f'{batch[\"prompt\"]}: {batch[\"original_text\"]}'\n",
    "        # batch[\"generated_text\"] = f'{batch[\"prompt\"]}: {batch[\"original_text\"]}' # self.llm.generate(prompt, sampling_params)\n",
    "        # print(batch)\n",
    "        outputs = self.llm.generate(batch[\"input\"], sampling_params)\n",
    "        generated_text = []\n",
    "        for output in outputs:\n",
    "            generated_text.append(' '.join([o.text for o in output.outputs]))\n",
    "        batch[\"generated_text\"] = generated_text\n",
    "        # batch[\"generated_text\"] = [output[0][\"generated_text\"] for output in outputs]\n",
    "        # prompt = []\n",
    "        # generated_text = []\n",
    "        # for output in outputs:\n",
    "        #     prompt.append(output.prompt)\n",
    "        #     generated_text.append(' '.join([o.text for o in output.outputs]))\n",
    "        # return {\n",
    "        #     \"original_text\": batch[\"original_text\"],\n",
    "        #     \"rewrite_prompt\": batch[\"prompt\"],\n",
    "        #     \"generated_text\": None,\n",
    "        # }\n",
    "        return batch\n",
    "\n",
    "ds = ds.map_batches(\n",
    "    LLMPredictor,\n",
    "    # Set the concurrency to the number of LLM instances.\n",
    "    concurrency=2,\n",
    "    # Specify the number of GPUs required per LLM instance.\n",
    "    # NOTE: Do NOT set `num_gpus` when using vLLM with tensor-parallelism\n",
    "    # (i.e., `tensor_parallel_size`).\n",
    "    num_gpus=1,\n",
    "    # Specify the batch size for inference.\n",
    "    batch_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-05 23:13:20,974\tINFO streaming_executor.py:115 -- Starting execution of Dataset. Full log is in /tmp/ray/session_2024-04-05_23-12-50_051505_6888/logs/ray-data.log\n",
      "2024-04-05 23:13:20,975\tINFO streaming_executor.py:116 -- Execution plan of Dataset: InputDataBuffer[Input] -> ActorPoolMapOperator[MapBatches(LLMPredictor)] -> TaskPoolMapOperator[Write]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=8286)\u001b[0m INFO 04-05 23:13:24 llm_engine.py:87] Initializing an LLM engine with config: model='google/gemma-7b-it', tokenizer='google/gemma-7b-it', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1500, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "\u001b[36m(_MapWorker pid=8286)\u001b[0m INFO 04-05 23:13:29 weight_utils.py:163] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(_MapWorker pid=8285)\u001b[0m INFO 04-05 23:13:24 llm_engine.py:87] Initializing an LLM engine with config: model='google/gemma-7b-it', tokenizer='google/gemma-7b-it', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1500, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "\u001b[36m(_MapWorker pid=8286)\u001b[0m INFO 04-05 23:13:34 llm_engine.py:357] # GPU blocks: 566, # CPU blocks: 585\n",
      "\u001b[36m(_MapWorker pid=8286)\u001b[0m INFO 04-05 23:13:37 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(_MapWorker pid=8286)\u001b[0m INFO 04-05 23:13:37 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(_MapWorker pid=8285)\u001b[0m INFO 04-05 23:13:29 weight_utils.py:163] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(_MapWorker pid=8286)\u001b[0m INFO 04-05 23:13:42 model_runner.py:756] Graph capturing finished in 5 secs.\n",
      "\u001b[36m(_MapWorker pid=8285)\u001b[0m INFO 04-05 23:13:44 llm_engine.py:357] # GPU blocks: 566, # CPU blocks: 585\n",
      "\u001b[36m(_MapWorker pid=8285)\u001b[0m INFO 04-05 23:13:47 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(_MapWorker pid=8285)\u001b[0m INFO 04-05 23:13:47 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(_MapWorker pid=8285)\u001b[0m INFO 04-05 23:13:52 model_runner.py:756] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50aa959a09604484ae05c7e0ad0703da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(LLMPredictor) 1:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99334853f125410fa872551e745d0b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Write 2:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb476255b2647e795dff8a7c3ccf502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(LLMPredictor)) pid=8286)\u001b[0m WARNING 04-05 23:13:53 scheduler.py:195] Input prompt (1762 tokens) is too long and exceeds limit of 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  25%|██▌       | 1/4 [00:00<00:01,  2.43it/s]\n",
      "Processed prompts:  50%|█████     | 2/4 [00:02<00:02,  1.26s/it]\n",
      "Processed prompts: 100%|██████████| 4/4 [00:05<00:00,  1.41s/it]\n",
      "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "Processed prompts:  25%|██▌       | 1/4 [00:02<00:07,  2.60s/it]\n",
      "Processed prompts: 100%|██████████| 4/4 [00:05<00:00,  1.44s/it]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Processed prompts:  25%|██▌       | 1/4 [00:02<00:08,  2.98s/it]\n",
      "Processed prompts: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ds.write_parquet(\"./data/exp_test/train_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pl.read_parquet(\"./data/exp_test/train_data/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.write_csv(\"./data/exp_test/train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.filter(\n",
    "    #pl.col('generated_text').str.contains('\\*\\*.*?\\*\\*'),\n",
    "    ~pl.col('generated_text').str.contains('I am unable'),\n",
    "    pl.col('generated_text').str.len_chars() > 0,\n",
    "    ~pl.col('generated_text').str.contains(pl.col('original_text'), literal=True),\n",
    "    ~pl.col('generated_text').str.contains('Sure', literal=True),\n",
    ").write_parquet('./data/exp_6/train_data/complete_1/complete_ds.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do some post processing\n",
    "\n",
    "import pathlib\n",
    "\n",
    "path: pathlib.Path =  \"./data/exp_6/train_data/complete/complete_ds.parquet\"\n",
    "\n",
    "path \n",
    "\n",
    "train_df = pl.read_parquet(\"./data/exp_6/train_data/*.parquet\")\n",
    "train_df.filter(\n",
    "    # pl.col('generated_text').str.contains('\\*\\*.*\\*\\*'),\n",
    "    ~pl.col('generated_text').str.contains('I am unable'),\n",
    "    pl.col('generated_text').str.len_chars() > 0,\n",
    "    ~pl.col('generated_text').str.contains(pl.col('original_text'), literal=True),\n",
    "    ~pl.col('generated_text').str.contains('Sure', literal=True),\n",
    ").write_parquet(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_promt_reversal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
